from flask import Flask, request, send_file, Response, send_from_directory, after_this_request
from flask_cors import CORS
import yt_dlp
import os
import sys
import tempfile
import logging
import argparse
import json
import re
import base64
import uuid
import threading
import time
import shutil
from pathlib import Path

app = Flask(__name__, static_folder='static', static_url_path='')
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def json_response(data, status=200):
    """è¿”å› JSON å“åº”ï¼Œæ”¯æŒä¸­æ–‡"""
    return Response(
        json.dumps(data, ensure_ascii=False),
        status=status,
        mimetype='application/json; charset=utf-8'
    )

# è§†é¢‘è´¨é‡ä¼˜å…ˆçº§ï¼š720p > 480p > 360p > 1080p > 4K
QUALITY_PRIORITY = ['720', '480', '360', '1080', '2160']

def ensure_cookies():
    """
    ä»ç¯å¢ƒå˜é‡æ¢å¤ cookiesï¼ˆå¦‚æœå­˜åœ¨ï¼‰

    æ£€æŸ¥ COOKIES_BASE64 ç¯å¢ƒå˜é‡ï¼Œå¦‚æœå­˜åœ¨åˆ™è§£ç å¹¶å†™å…¥ cookies.txt
    è¿™å…è®¸é€šè¿‡ç¯å¢ƒå˜é‡æ›´æ–° cookiesï¼Œè€Œä¸éœ€è¦é‡æ–°æ„å»ºé•œåƒ

    è¿”å› cookies æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæˆåŠŸï¼‰æˆ– None
    """
    # é»˜è®¤ cookies æ–‡ä»¶è·¯å¾„
    default_cookies_path = os.environ.get('COOKIES_FILE') or '/app/cookies.txt'

    # æ£€æŸ¥æ˜¯å¦æœ‰ç¯å¢ƒå˜é‡ä¸­çš„ base64 ç¼–ç  cookies
    cookies_base64 = os.environ.get('COOKIES_BASE64')

    if cookies_base64:
        try:
            logger.info('ğŸª ä»ç¯å¢ƒå˜é‡ COOKIES_BASE64 æ¢å¤ cookies...')

            # è§£ç  base64
            cookies_content = base64.b64decode(cookies_base64).decode('utf-8')

            # ç»Ÿè®¡ cookies æ•°é‡ï¼ˆéç©ºè¡Œä¸”éæ³¨é‡Šè¡Œï¼‰
            cookie_lines = [line for line in cookies_content.split('\n')
                          if line.strip() and not line.strip().startswith('#')]
            cookie_count = len(cookie_lines)

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            cookies_dir = os.path.dirname(default_cookies_path)
            if cookies_dir and not os.path.exists(cookies_dir):
                os.makedirs(cookies_dir, exist_ok=True)

            # å†™å…¥æ–‡ä»¶
            with open(default_cookies_path, 'w', encoding='utf-8') as f:
                f.write(cookies_content)

            logger.info(f'âœ… Cookies å·²ä»ç¯å¢ƒå˜é‡æ¢å¤åˆ° {default_cookies_path}')
            logger.info(f'ğŸ“Š å…± {cookie_count} ä¸ª cookies')

            return default_cookies_path

        except Exception as e:
            logger.error(f'âš ï¸ ä»ç¯å¢ƒå˜é‡æ¢å¤ cookies å¤±è´¥: {e}')
            import traceback
            traceback.print_exc(file=sys.stderr)

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æŒ‚è½½çš„ cookies æ–‡ä»¶
    if os.path.exists(default_cookies_path):
        logger.info(f'âœ… ä½¿ç”¨ç°æœ‰ cookies: {default_cookies_path}')
        return default_cookies_path

    logger.warning('âš ï¸ æœªæ‰¾åˆ° cookies æ–‡ä»¶ï¼Œä¹Ÿæ²¡æœ‰ COOKIES_BASE64 ç¯å¢ƒå˜é‡')
    return None

# å…¨å±€å˜é‡ï¼šcookies æ–‡ä»¶è·¯å¾„
# ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œç”¨äº gunicorn å¯åŠ¨
# å¯åŠ¨æ—¶å°è¯•ä»ç¯å¢ƒå˜é‡æ¢å¤ cookies
COOKIES_FILE = ensure_cookies()

# å…¨å±€å˜é‡ï¼šä»£ç†è®¾ç½®
# ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œç”¨äº gunicorn å¯åŠ¨
PROXY_URL = os.environ.get('PROXY_URL', None)

# åˆå§‹åŒ–æ—¶æ£€æŸ¥ä»£ç†è®¾ç½®
if PROXY_URL:
    logger.info(f'å·²é…ç½®ä»£ç†: {PROXY_URL}')
else:
    logger.info('æœªé…ç½®ä»£ç†ï¼Œå°†ç›´æ¥è¿æ¥')

# ================ ä¸‹è½½ä»»åŠ¡ç®¡ç† ================
# ä½¿ç”¨åŸºäºæ–‡ä»¶çš„ä»»åŠ¡å­˜å‚¨ï¼Œè§£å†³å¤š worker è¿›ç¨‹é—´æ•°æ®å…±äº«é—®é¢˜
# æ¯ä¸ªä»»åŠ¡å­˜å‚¨ä¸ºç‹¬ç«‹çš„ JSON æ–‡ä»¶ï¼š/tmp/yt-dlp-tasks/{task_id}.json

# ç¼“å­˜ç›®å½•
CACHE_DIR = os.environ.get('CACHE_DIR', '/tmp/yt-dlp-cache')
os.makedirs(CACHE_DIR, exist_ok=True)

# ä»»åŠ¡æ–‡ä»¶ç›®å½•
TASKS_DIR = os.path.join(CACHE_DIR, 'tasks')
os.makedirs(TASKS_DIR, exist_ok=True)

# æ–‡ä»¶è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
FILE_EXPIRE_TIME = 5 * 60  # 5åˆ†é’Ÿ

# æ–‡ä»¶é”ç›®å½•
LOCKS_DIR = os.path.join(CACHE_DIR, 'locks')
os.makedirs(LOCKS_DIR, exist_ok=True)

import fcntl

def get_task_file_path(task_id):
    """è·å–ä»»åŠ¡æ–‡ä»¶è·¯å¾„"""
    return os.path.join(TASKS_DIR, f'{task_id}.json')

def get_lock_file_path(task_id):
    """è·å–é”æ–‡ä»¶è·¯å¾„"""
    return os.path.join(LOCKS_DIR, f'{task_id}.lock')

def save_task(task_id, task_data):
    """ä¿å­˜ä»»åŠ¡æ•°æ®åˆ°æ–‡ä»¶"""
    task_file = get_task_file_path(task_id)
    lock_file = get_lock_file_path(task_id)

    try:
        with open(lock_file, 'w') as lf:
            fcntl.flock(lf.fileno(), fcntl.LOCK_EX)
            try:
                with open(task_file, 'w', encoding='utf-8') as f:
                    json.dump(task_data, f, ensure_ascii=False)
            finally:
                fcntl.flock(lf.fileno(), fcntl.LOCK_UN)
    except Exception as e:
        logger.error(f'ä¿å­˜ä»»åŠ¡æ•°æ®å¤±è´¥: {task_id}, é”™è¯¯: {e}')

def load_task(task_id):
    """ä»æ–‡ä»¶åŠ è½½ä»»åŠ¡æ•°æ®"""
    task_file = get_task_file_path(task_id)
    lock_file = get_lock_file_path(task_id)

    if not os.path.exists(task_file):
        return None

    try:
        with open(lock_file, 'w') as lf:
            fcntl.flock(lf.fileno(), fcntl.LOCK_SH)
            try:
                with open(task_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            finally:
                fcntl.flock(lf.fileno(), fcntl.LOCK_UN)
    except Exception as e:
        logger.error(f'åŠ è½½ä»»åŠ¡æ•°æ®å¤±è´¥: {task_id}, é”™è¯¯: {e}')
        return None

def update_task(task_id, updates):
    """æ›´æ–°ä»»åŠ¡æ•°æ®"""
    task_file = get_task_file_path(task_id)
    lock_file = get_lock_file_path(task_id)

    try:
        with open(lock_file, 'w') as lf:
            fcntl.flock(lf.fileno(), fcntl.LOCK_EX)
            try:
                task_data = {}
                if os.path.exists(task_file):
                    with open(task_file, 'r', encoding='utf-8') as f:
                        task_data = json.load(f)

                task_data.update(updates)

                with open(task_file, 'w', encoding='utf-8') as f:
                    json.dump(task_data, f, ensure_ascii=False)
            finally:
                fcntl.flock(lf.fileno(), fcntl.LOCK_UN)
    except Exception as e:
        logger.error(f'æ›´æ–°ä»»åŠ¡æ•°æ®å¤±è´¥: {task_id}, é”™è¯¯: {e}')

def delete_task(task_id):
    """åˆ é™¤ä»»åŠ¡æ–‡ä»¶"""
    task_file = get_task_file_path(task_id)
    lock_file = get_lock_file_path(task_id)

    try:
        if os.path.exists(task_file):
            os.remove(task_file)
        if os.path.exists(lock_file):
            os.remove(lock_file)
    except Exception as e:
        logger.error(f'åˆ é™¤ä»»åŠ¡æ–‡ä»¶å¤±è´¥: {task_id}, é”™è¯¯: {e}')

def get_all_task_ids():
    """è·å–æ‰€æœ‰ä»»åŠ¡ID"""
    try:
        task_files = os.listdir(TASKS_DIR)
        return [f[:-5] for f in task_files if f.endswith('.json')]
    except Exception as e:
        logger.error(f'è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}')
        return []

def cleanup_expired_files():
    """æ¸…ç†è¿‡æœŸçš„ä¸‹è½½æ–‡ä»¶"""
    while True:
        try:
            time.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
            current_time = time.time()
            tasks_to_remove = []

            # éå†æ‰€æœ‰ä»»åŠ¡æ–‡ä»¶
            for task_id in get_all_task_ids():
                task = load_task(task_id)
                if not task:
                    continue

                # è·³è¿‡æ­£åœ¨ä¸‹è½½çš„ä»»åŠ¡
                if task['status'] == 'downloading':
                    continue

                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸï¼ˆä¸‹è½½å®Œæˆå5åˆ†é’Ÿæœªè¢«ä¸‹è½½ï¼Œæˆ–å·²è¢«ä¸‹è½½è¿‡ï¼‰
                if task['status'] == 'completed':
                    # å·²è¢«ä¸‹è½½è¿‡ï¼Œåˆ é™¤æ–‡ä»¶å’Œä»»åŠ¡
                    if task.get('download_count', 0) > 0:
                        tasks_to_remove.append(task_id)
                        logger.info(f'ä»»åŠ¡ {task_id} å·²è¢«ä¸‹è½½ï¼Œå‡†å¤‡æ¸…ç†')
                    # è¶…è¿‡5åˆ†é’Ÿæœªä¸‹è½½
                    elif task.get('downloaded_at') and (current_time - task['downloaded_at'] > FILE_EXPIRE_TIME):
                        tasks_to_remove.append(task_id)
                        logger.info(f'ä»»åŠ¡ {task_id} å·²è¿‡æœŸï¼ˆ5åˆ†é’Ÿæœªä¸‹è½½ï¼‰ï¼Œå‡†å¤‡æ¸…ç†')

                # å¤±è´¥çš„ä»»åŠ¡ä¹Ÿæ¸…ç†
                elif task['status'] == 'failed':
                    if current_time - task.get('created_at', 0) > FILE_EXPIRE_TIME:
                        tasks_to_remove.append(task_id)

            # æ¸…ç†ä»»åŠ¡å’Œæ–‡ä»¶
            for task_id in tasks_to_remove:
                task = load_task(task_id)
                if task:
                    filepath = task.get('filepath')
                    temp_dir = task.get('temp_dir')

                    # åˆ é™¤æ–‡ä»¶
                    if filepath and os.path.exists(filepath):
                        try:
                            os.remove(filepath)
                            logger.info(f'å·²åˆ é™¤æ–‡ä»¶: {filepath}')
                        except Exception as e:
                            logger.error(f'åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}')

                    # åˆ é™¤ä¸´æ—¶ç›®å½•
                    if temp_dir and os.path.exists(temp_dir):
                        try:
                            shutil.rmtree(temp_dir)
                            logger.info(f'å·²åˆ é™¤ä¸´æ—¶ç›®å½•: {temp_dir}')
                        except Exception as e:
                            logger.error(f'åˆ é™¤ä¸´æ—¶ç›®å½•å¤±è´¥: {e}')

                    # åˆ é™¤ä»»åŠ¡æ–‡ä»¶
                    delete_task(task_id)
                    logger.info(f'å·²æ¸…ç†ä»»åŠ¡: {task_id}')

        except Exception as e:
            logger.error(f'æ¸…ç†çº¿ç¨‹é”™è¯¯: {e}')

# å¯åŠ¨æ¸…ç†çº¿ç¨‹
cleanup_thread = threading.Thread(target=cleanup_expired_files, daemon=True)
cleanup_thread.start()
logger.info('å·²å¯åŠ¨æ–‡ä»¶æ¸…ç†çº¿ç¨‹')

def sanitize_filename(filename, max_length=100):
    """
    æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦
    """
    # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
    filename = re.sub(r'[<>:"/\\|?*]', '', filename)
    # ç§»é™¤æ§åˆ¶å­—ç¬¦
    filename = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', filename)
    # å»é™¤é¦–å°¾ç©ºæ ¼
    filename = filename.strip()
    # é™åˆ¶é•¿åº¦ï¼ˆè€ƒè™‘åˆ°æ‰©å±•å .mp4 å  4 ä¸ªå­—ç¬¦ï¼‰
    if len(filename) > max_length - 4:
        filename = filename[:max_length - 4]
    # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åç§°
    if not filename:
        filename = 'video'
    return filename

def get_format_selector():
    """
    æ ¹æ®ä¼˜å…ˆçº§è¿”å›æ ¼å¼é€‰æ‹©å™¨
    ä¼˜å…ˆé€‰æ‹©å·²åŒ…å«éŸ³è§†é¢‘çš„å•ä¸€æ ¼å¼ï¼Œé¿å…åˆå¹¶æ“ä½œï¼ˆå¤§å¹…æå‡ä¸‹è½½é€Ÿåº¦ï¼‰
    ä¼˜å…ˆ 720pï¼Œå…¶æ¬¡ 480pï¼Œç„¶å 360pã€1080pã€æœ€åæœ€ä½³
    """
    # ä¼˜å…ˆé€‰æ‹©å·²ç»åŒ…å«éŸ³è§†é¢‘çš„æ ¼å¼ï¼Œé¿å… ffmpeg åˆå¹¶ï¼ˆé€Ÿåº¦æå‡ 2-3 å€ï¼‰
    return 'best[height<=720]/best[height<=480]/best[height<=360]/best[height<=1080]/best'

@app.route('/')
def index():
    """è¿”å›å‰ç«¯é¡µé¢"""
    return send_from_directory('static', 'index.html')

@app.route('/health')
def health():
    """Koyeb å¥åº·æ£€æŸ¥"""
    return json_response({'status': 'healthy'})

@app.route('/api/download', methods=['POST'])
def download_video():
    """
    ä¸‹è½½ YouTube è§†é¢‘
    è¯·æ±‚ä½“: {
        "url": "YouTubeè§†é¢‘URL",
        "format_id": "æ ¼å¼IDï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™è‡ªåŠ¨é€‰æ‹©æœ€ä½³ï¼‰",
        "subtitle": "å­—å¹•è¯­è¨€ä»£ç ï¼ˆå¯é€‰ï¼‰"
    }
    """
    try:
        data = request.get_json()

        if not data or 'url' not in data:
            return json_response({'error': 'ç¼ºå°‘ URL å‚æ•°'}, 400)

        video_url = data['url']
        format_id = data.get('format_id')
        subtitle_lang = data.get('subtitle')

        logger.info(f'å¼€å§‹ä¸‹è½½è§†é¢‘: {video_url}, æ ¼å¼: {format_id}, å­—å¹•: {subtitle_lang}')

        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp()
        output_template = os.path.join(temp_dir, '%(title)s.%(ext)s')

        # é…ç½® yt-dlp é€‰é¡¹
        ydl_opts = {
            'outtmpl': output_template,
            'quiet': False,
            'no_warnings': False,
            'extract_flat': False,
            'nocheckcertificate': True,
        }

        # è®¾ç½®æ ¼å¼ï¼šå¦‚æœç”¨æˆ·æŒ‡å®šäº† format_id åˆ™ä½¿ç”¨ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤é€‰æ‹©å™¨
        if format_id:
            # ç”¨æˆ·æŒ‡å®šçš„æ ¼å¼å¯èƒ½æ²¡æœ‰éŸ³é¢‘ï¼Œéœ€è¦åˆå¹¶æœ€ä½³éŸ³é¢‘
            ydl_opts['format'] = f'{format_id}+bestaudio/best/{format_id}'
            ydl_opts['merge_output_format'] = 'mp4'
        else:
            ydl_opts['format'] = get_format_selector()

        # é…ç½®å­—å¹•ä¸‹è½½
        if subtitle_lang:
            ydl_opts['writesubtitles'] = True
            ydl_opts['subtitleslangs'] = [subtitle_lang]
            ydl_opts['writeautomaticsub'] = True

        if COOKIES_FILE and os.path.exists(COOKIES_FILE):
            ydl_opts['cookiefile'] = COOKIES_FILE
            logger.info(f'ä½¿ç”¨ cookies æ–‡ä»¶: {COOKIES_FILE}')
        else:
            logger.warning('æœªé…ç½®æˆ–æœªæ‰¾åˆ° cookies.txt æ–‡ä»¶')

        if PROXY_URL:
            ydl_opts['proxy'] = PROXY_URL
            logger.info(f'ä½¿ç”¨ä»£ç†: {PROXY_URL}')

        # ä¸‹è½½è§†é¢‘
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(video_url, download=True)

            # è·å–ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„
            if 'requested_downloads' in info:
                video_file = info['requested_downloads'][0]['filepath']
            else:
                video_file = ydl.prepare_filename(info)

            if not os.path.exists(video_file):
                logger.error(f'è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_file}')
                return json_response({'error': 'è§†é¢‘ä¸‹è½½å¤±è´¥'}, 500)

            video_title = info.get('title', 'video')
            video_ext = info.get('ext', 'mp4')
            file_size = os.path.getsize(video_file)

            clean_title = sanitize_filename(video_title)
            final_filename = f'{clean_title}.{video_ext}'

            logger.info(f'è§†é¢‘ä¸‹è½½æˆåŠŸ: {video_title}, å¤§å°: {file_size / 1024 / 1024:.2f} MB')

            # æ£€æŸ¥æ˜¯å¦æœ‰å­—å¹•æ–‡ä»¶éœ€è¦æ‰“åŒ…
            subtitle_file = None
            if subtitle_lang:
                # æŸ¥æ‰¾å­—å¹•æ–‡ä»¶
                for ext in ['vtt', 'srt', 'ass']:
                    sub_path = os.path.join(temp_dir, f'{os.path.splitext(os.path.basename(video_file))[0]}.{subtitle_lang}.{ext}')
                    if os.path.exists(sub_path):
                        subtitle_file = sub_path
                        break

            # å¦‚æœæœ‰å­—å¹•ï¼Œæ‰“åŒ…æˆ zip
            if subtitle_file:
                import zipfile
                zip_filename = f'{clean_title}.zip'
                zip_path = os.path.join(temp_dir, zip_filename)

                with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
                    zf.write(video_file, final_filename)
                    sub_ext = os.path.splitext(subtitle_file)[1]
                    zf.write(subtitle_file, f'{clean_title}.{subtitle_lang}{sub_ext}')

                logger.info(f'æ‰“åŒ…è§†é¢‘å’Œå­—å¹•: {zip_filename}')
                return send_file(
                    zip_path,
                    as_attachment=True,
                    download_name=zip_filename,
                    mimetype='application/zip'
                )

            # æ²¡æœ‰å­—å¹•ï¼Œç›´æ¥è¿”å›è§†é¢‘
            mimetype_map = {
                'mp4': 'video/mp4',
                'webm': 'video/webm',
                'mkv': 'video/x-matroska',
                'avi': 'video/x-msvideo',
                'mov': 'video/quicktime',
            }
            mimetype = mimetype_map.get(video_ext, 'application/octet-stream')

            return send_file(
                video_file,
                as_attachment=True,
                download_name=final_filename,
                mimetype=mimetype
            )

    except yt_dlp.utils.DownloadError as e:
        logger.error(f'ä¸‹è½½é”™è¯¯: {str(e)}')
        return json_response({'error': f'ä¸‹è½½å¤±è´¥: {str(e)}'}, 400)
    except Exception as e:
        logger.error(f'æœåŠ¡å™¨é”™è¯¯: {str(e)}')
        return json_response({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}, 500)


# ================ å¼‚æ­¥ä¸‹è½½ç›¸å…³æ¥å£ ================

def download_video_task(task_id, video_url, format_id, subtitle_lang):
    """åå°ä¸‹è½½è§†é¢‘çš„ä»»åŠ¡å‡½æ•°"""
    temp_dir = None
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp(dir=CACHE_DIR)
        output_template = os.path.join(temp_dir, '%(title)s.%(ext)s')

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        update_task(task_id, {
            'temp_dir': temp_dir,
            'status': 'downloading'
        })

        # è¿›åº¦å›è°ƒå‡½æ•°
        def progress_hook(d):
            task = load_task(task_id)
            if not task:
                return

            if d['status'] == 'downloading':
                total = d.get('total_bytes') or d.get('total_bytes_estimate') or 0
                downloaded = d.get('downloaded_bytes', 0)

                if total > 0:
                    progress = (downloaded / total) * 100
                else:
                    progress = 0

                update_task(task_id, {
                    'progress': round(progress, 1),
                    'downloaded_bytes': downloaded,
                    'total_bytes': total,
                    'speed': d.get('speed', 0),
                    'eta': d.get('eta', 0),
                })
            elif d['status'] == 'finished':
                update_task(task_id, {
                    'progress': 100,
                    'status': 'processing'
                })

        # é…ç½® yt-dlp é€‰é¡¹
        ydl_opts = {
            'outtmpl': output_template,
            'quiet': True,
            'no_warnings': True,
            'extract_flat': False,
            'nocheckcertificate': True,
            'progress_hooks': [progress_hook],
        }

        # è®¾ç½®æ ¼å¼
        if format_id:
            ydl_opts['format'] = f'{format_id}+bestaudio/best/{format_id}'
            ydl_opts['merge_output_format'] = 'mp4'
        else:
            ydl_opts['format'] = get_format_selector()

        # é…ç½®å­—å¹•ä¸‹è½½
        if subtitle_lang:
            ydl_opts['writesubtitles'] = True
            ydl_opts['subtitleslangs'] = [subtitle_lang]
            ydl_opts['writeautomaticsub'] = True

        if COOKIES_FILE and os.path.exists(COOKIES_FILE):
            ydl_opts['cookiefile'] = COOKIES_FILE

        if PROXY_URL:
            ydl_opts['proxy'] = PROXY_URL

        # ä¸‹è½½è§†é¢‘
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(video_url, download=True)

            # è·å–ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„
            if 'requested_downloads' in info:
                video_file = info['requested_downloads'][0]['filepath']
            else:
                video_file = ydl.prepare_filename(info)

            if not os.path.exists(video_file):
                raise Exception('è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨')

            video_title = info.get('title', 'video')
            video_ext = info.get('ext', 'mp4')
            file_size = os.path.getsize(video_file)

            clean_title = sanitize_filename(video_title)
            final_filename = f'{clean_title}.{video_ext}'
            final_filepath = video_file
            final_mimetype = 'video/mp4'

            # æ£€æŸ¥æ˜¯å¦æœ‰å­—å¹•æ–‡ä»¶éœ€è¦æ‰“åŒ…
            subtitle_file = None
            if subtitle_lang:
                for ext in ['vtt', 'srt', 'ass']:
                    sub_path = os.path.join(temp_dir, f'{os.path.splitext(os.path.basename(video_file))[0]}.{subtitle_lang}.{ext}')
                    if os.path.exists(sub_path):
                        subtitle_file = sub_path
                        break

            # å¦‚æœæœ‰å­—å¹•ï¼Œæ‰“åŒ…æˆ zip
            if subtitle_file:
                import zipfile
                zip_filename = f'{clean_title}.zip'
                zip_path = os.path.join(temp_dir, zip_filename)

                with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
                    zf.write(video_file, final_filename)
                    sub_ext = os.path.splitext(subtitle_file)[1]
                    zf.write(subtitle_file, f'{clean_title}.{subtitle_lang}{sub_ext}')

                final_filename = zip_filename
                final_filepath = zip_path
                final_mimetype = 'application/zip'
                file_size = os.path.getsize(zip_path)

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
            update_task(task_id, {
                'status': 'completed',
                'progress': 100,
                'filename': final_filename,
                'filepath': final_filepath,
                'filesize': file_size,
                'mimetype': final_mimetype,
                'downloaded_at': time.time(),
                'download_count': 0,
            })

            logger.info(f'ä»»åŠ¡ {task_id} ä¸‹è½½å®Œæˆ: {final_filename}, å¤§å°: {file_size / 1024 / 1024:.2f} MB')

    except Exception as e:
        logger.error(f'ä»»åŠ¡ {task_id} ä¸‹è½½å¤±è´¥: {str(e)}')
        update_task(task_id, {
            'status': 'failed',
            'error': str(e),
        })
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if temp_dir and os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
            except:
                pass


@app.route('/api/start-download', methods=['POST'])
def start_download():
    """
    å¯åŠ¨åå°ä¸‹è½½ä»»åŠ¡
    è¯·æ±‚ä½“: {
        "url": "YouTubeè§†é¢‘URL",
        "format_id": "æ ¼å¼IDï¼ˆå¯é€‰ï¼‰",
        "subtitle": "å­—å¹•è¯­è¨€ä»£ç ï¼ˆå¯é€‰ï¼‰"
    }
    è¿”å›: { "task_id": "ä»»åŠ¡ID" }
    """
    try:
        data = request.get_json()

        if not data or 'url' not in data:
            return json_response({'error': 'ç¼ºå°‘ URL å‚æ•°'}, 400)

        video_url = data['url']
        format_id = data.get('format_id')
        subtitle_lang = data.get('subtitle')

        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())

        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€ï¼ˆä¿å­˜åˆ°æ–‡ä»¶ï¼‰
        save_task(task_id, {
            'status': 'pending',
            'progress': 0,
            'downloaded_bytes': 0,
            'total_bytes': 0,
            'speed': 0,
            'eta': 0,
            'filename': None,
            'filepath': None,
            'error': None,
            'created_at': time.time(),
            'downloaded_at': None,
            'download_count': 0,
            'temp_dir': None,
        })

        # å¯åŠ¨ä¸‹è½½çº¿ç¨‹
        thread = threading.Thread(
            target=download_video_task,
            args=(task_id, video_url, format_id, subtitle_lang),
            daemon=True
        )
        thread.start()

        logger.info(f'å¯åŠ¨ä¸‹è½½ä»»åŠ¡: {task_id}, URL: {video_url}')

        return json_response({'task_id': task_id})

    except Exception as e:
        logger.error(f'å¯åŠ¨ä¸‹è½½ä»»åŠ¡å¤±è´¥: {str(e)}')
        return json_response({'error': f'å¯åŠ¨ä¸‹è½½å¤±è´¥: {str(e)}'}, 500)


@app.route('/api/progress/<task_id>', methods=['GET'])
def get_progress(task_id):
    """
    è·å–ä¸‹è½½è¿›åº¦
    è¿”å›: {
        "status": "pending|downloading|processing|completed|failed",
        "progress": 0-100,
        "speed": ä¸‹è½½é€Ÿåº¦(bytes/s),
        "eta": é¢„è®¡å‰©ä½™æ—¶é—´(ç§’),
        "filename": æ–‡ä»¶å(å®Œæˆæ—¶),
        "filesize": æ–‡ä»¶å¤§å°(å®Œæˆæ—¶),
        "error": é”™è¯¯ä¿¡æ¯(å¤±è´¥æ—¶)
    }
    """
    task = load_task(task_id)

    if not task:
        return json_response({'error': 'ä»»åŠ¡ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ'}, 404)

    # æ£€æŸ¥æ˜¯å¦å·²è¢«ä¸‹è½½è¿‡
    if task['status'] == 'completed' and task.get('download_count', 0) > 0:
        return json_response({
            'status': 'expired',
            'error': 'æ–‡ä»¶å·²è¢«ä¸‹è½½ï¼Œä¸å¯é‡å¤ä¸‹è½½'
        })

    response = {
        'status': task['status'],
        'progress': task['progress'],
    }

    if task['status'] == 'downloading':
        response.update({
            'downloaded_bytes': task.get('downloaded_bytes', 0),
            'total_bytes': task.get('total_bytes', 0),
            'speed': task.get('speed', 0),
            'eta': task.get('eta', 0),
        })
    elif task['status'] == 'completed':
        response.update({
            'filename': task.get('filename'),
            'filesize': task.get('filesize'),
        })
    elif task['status'] == 'failed':
        response['error'] = task.get('error', 'æœªçŸ¥é”™è¯¯')

    return json_response(response)


@app.route('/api/file/<task_id>', methods=['GET'])
def download_file(task_id):
    """
    ä¸‹è½½å·²å®Œæˆçš„æ–‡ä»¶
    æ–‡ä»¶ä¸‹è½½åä¼šè¢«æ ‡è®°ï¼Œç¨åè‡ªåŠ¨æ¸…ç†
    """
    task = load_task(task_id)

    if not task:
        return json_response({'error': 'ä»»åŠ¡ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ'}, 404)

    if task['status'] != 'completed':
        return json_response({'error': 'æ–‡ä»¶å°šæœªå‡†å¤‡å¥½'}, 400)

    # æ£€æŸ¥æ˜¯å¦å·²è¢«ä¸‹è½½è¿‡
    if task.get('download_count', 0) > 0:
        return json_response({'error': 'æ–‡ä»¶å·²è¢«ä¸‹è½½ï¼Œä¸å¯é‡å¤ä¸‹è½½'}, 410)

    filepath = task.get('filepath')
    filename = task.get('filename')
    mimetype = task.get('mimetype', 'application/octet-stream')

    if not filepath or not os.path.exists(filepath):
        return json_response({'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}, 404)

    # æ ‡è®°ä¸ºå·²ä¸‹è½½
    update_task(task_id, {
        'download_count': task.get('download_count', 0) + 1
    })

    logger.info(f'ç”¨æˆ·ä¸‹è½½æ–‡ä»¶: {task_id} - {filename}')

    return send_file(
        filepath,
        as_attachment=True,
        download_name=filename,
        mimetype=mimetype
    )


@app.route('/api/info', methods=['POST'])
def get_video_info():
    """
    è·å–è§†é¢‘ä¿¡æ¯ï¼ˆä¸ä¸‹è½½ï¼‰
    è¯·æ±‚ä½“: {"url": "YouTubeè§†é¢‘URL"}
    è¿”å›: è§†é¢‘åŸºæœ¬ä¿¡æ¯ã€å¯ç”¨æ ¼å¼åˆ—è¡¨ã€å­—å¹•åˆ—è¡¨
    """
    try:
        data = request.get_json()

        if not data or 'url' not in data:
            return json_response({'error': 'ç¼ºå°‘ URL å‚æ•°'}, 400)

        video_url = data['url']
        logger.info(f'è·å–è§†é¢‘ä¿¡æ¯: {video_url}')

        ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'extract_flat': False,
            'nocheckcertificate': True,
            'writesubtitles': True,
            'allsubtitles': True,
        }

        if COOKIES_FILE and os.path.exists(COOKIES_FILE):
            ydl_opts['cookiefile'] = COOKIES_FILE

        if PROXY_URL:
            ydl_opts['proxy'] = PROXY_URL

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(video_url, download=False)

            # æå–å¯ç”¨çš„è§†é¢‘æ ¼å¼
            formats = []
            seen_resolutions = set()

            for fmt in info.get('formats', []):
                # åªå¤„ç†åŒ…å«è§†é¢‘çš„æ ¼å¼
                if fmt.get('vcodec') == 'none':
                    continue

                height = fmt.get('height')
                if not height:
                    continue

                format_id = fmt.get('format_id')
                ext = fmt.get('ext', 'mp4')
                filesize = fmt.get('filesize') or fmt.get('filesize_approx')
                vcodec = fmt.get('vcodec', '')
                acodec = fmt.get('acodec', '')
                fps = fmt.get('fps')

                # æ„å»ºåˆ†è¾¨ç‡æ ‡ç­¾
                resolution_label = f"{height}p"
                if fps and fps > 30:
                    resolution_label += f" {fps}fps"

                # åˆ¤æ–­æ˜¯å¦åŒ…å«éŸ³é¢‘
                has_audio = acodec and acodec != 'none'

                # ç”¨äºå»é‡çš„ keyï¼ˆåŒåˆ†è¾¨ç‡+fpsåªä¿ç•™ä¸€ä¸ªï¼‰
                dedup_key = f"{height}_{fps}_{has_audio}"
                if dedup_key in seen_resolutions:
                    continue
                seen_resolutions.add(dedup_key)

                formats.append({
                    'format_id': format_id,
                    'height': height,
                    'resolution': resolution_label,
                    'ext': ext,
                    'filesize': filesize,
                    'has_audio': has_audio,
                    'vcodec': vcodec.split('.')[0] if vcodec else '',
                    'acodec': acodec.split('.')[0] if acodec else '',
                    'fps': fps,
                })

            # æŒ‰åˆ†è¾¨ç‡ä»é«˜åˆ°ä½æ’åº
            formats.sort(key=lambda x: (x['height'], x.get('fps') or 0), reverse=True)

            # æå–å¯ç”¨å­—å¹•
            subtitles = []
            subtitle_data = info.get('subtitles', {})
            auto_captions = info.get('automatic_captions', {})

            # æ‰‹åŠ¨ä¸Šä¼ çš„å­—å¹•
            for lang, subs in subtitle_data.items():
                if subs:
                    subtitles.append({
                        'lang': lang,
                        'name': get_language_name(lang),
                        'auto': False,
                    })

            # è‡ªåŠ¨ç”Ÿæˆçš„å­—å¹•
            for lang, subs in auto_captions.items():
                if subs and lang not in subtitle_data:
                    subtitles.append({
                        'lang': lang,
                        'name': get_language_name(lang) + ' (è‡ªåŠ¨ç”Ÿæˆ)',
                        'auto': True,
                    })

            result = {
                'title': info.get('title'),
                'duration': info.get('duration'),
                'thumbnail': info.get('thumbnail'),
                'uploader': info.get('uploader'),
                'view_count': info.get('view_count'),
                'description': info.get('description', '')[:200],
                'formats': formats,
                'subtitles': subtitles,
            }

            return Response(
                json.dumps(result, ensure_ascii=False),
                mimetype='application/json; charset=utf-8'
            )

    except Exception as e:
        logger.error(f'è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {str(e)}')
        return json_response({'error': f'è·å–ä¿¡æ¯å¤±è´¥: {str(e)}'}, 400)


def get_language_name(lang_code):
    """å°†è¯­è¨€ä»£ç è½¬æ¢ä¸ºå¯è¯»åç§°"""
    lang_map = {
        'zh-Hans': 'ä¸­æ–‡(ç®€ä½“)',
        'zh-Hant': 'ä¸­æ–‡(ç¹ä½“)',
        'zh-CN': 'ä¸­æ–‡(ç®€ä½“)',
        'zh-TW': 'ä¸­æ–‡(ç¹ä½“)',
        'zh': 'ä¸­æ–‡',
        'en': 'è‹±è¯­',
        'en-US': 'è‹±è¯­(ç¾å›½)',
        'en-GB': 'è‹±è¯­(è‹±å›½)',
        'ja': 'æ—¥è¯­',
        'ko': 'éŸ©è¯­',
        'es': 'è¥¿ç­ç‰™è¯­',
        'fr': 'æ³•è¯­',
        'de': 'å¾·è¯­',
        'ru': 'ä¿„è¯­',
        'pt': 'è‘¡è„ç‰™è¯­',
        'it': 'æ„å¤§åˆ©è¯­',
        'ar': 'é˜¿æ‹‰ä¼¯è¯­',
        'hi': 'å°åœ°è¯­',
        'th': 'æ³°è¯­',
        'vi': 'è¶Šå—è¯­',
        'id': 'å°å°¼è¯­',
        'ms': 'é©¬æ¥è¯­',
        'tr': 'åœŸè€³å…¶è¯­',
        'pl': 'æ³¢å…°è¯­',
        'nl': 'è·å…°è¯­',
        'sv': 'ç‘å…¸è¯­',
        'fi': 'èŠ¬å…°è¯­',
        'no': 'æŒªå¨è¯­',
        'da': 'ä¸¹éº¦è¯­',
        'cs': 'æ·å…‹è¯­',
        'hu': 'åŒˆç‰™åˆ©è¯­',
        'el': 'å¸Œè…Šè¯­',
        'he': 'å¸Œä¼¯æ¥è¯­',
        'uk': 'ä¹Œå…‹å…°è¯­',
        'ro': 'ç½—é©¬å°¼äºšè¯­',
    }
    return lang_map.get(lang_code, lang_code)

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='YouTube Downloader API Server')
    parser.add_argument(
        '--cookies',
        type=str,
        default='/app/cookies.txt',
        help='cookies.txt æ–‡ä»¶è·¯å¾„ (é»˜è®¤: /app/cookies.txt)'
    )
    parser.add_argument(
        '--port',
        type=int,
        default=None,
        help='æœåŠ¡å™¨ç«¯å£ (é»˜è®¤: ä»ç¯å¢ƒå˜é‡ PORT è¯»å–ï¼Œæˆ– 8000)'
    )
    parser.add_argument(
        '--proxy',
        type=str,
        default=None,
        help='ä»£ç†æœåŠ¡å™¨åœ°å€ (ä¾‹å¦‚: socks5://127.0.0.1:1080 æˆ– http://proxy.example.com:8080)'
    )
    return parser.parse_args()

if __name__ == '__main__':
    args = parse_args()

    # è®¾ç½®å…¨å±€ cookies æ–‡ä»¶è·¯å¾„
    COOKIES_FILE = args.cookies
    if os.path.exists(COOKIES_FILE):
        logger.info(f'å·²é…ç½® cookies æ–‡ä»¶: {COOKIES_FILE}')
    else:
        logger.warning(f'Cookies æ–‡ä»¶ä¸å­˜åœ¨: {COOKIES_FILE}ï¼Œå°†åœ¨æ²¡æœ‰ cookies çš„æƒ…å†µä¸‹è¿è¡Œ')

    # è®¾ç½®å…¨å±€ä»£ç†
    if args.proxy:
        PROXY_URL = args.proxy
        logger.info(f'å·²é…ç½®ä»£ç†: {PROXY_URL}')
    else:
        logger.info('æœªé…ç½®ä»£ç†ï¼Œå°†ç›´æ¥è¿æ¥')

    # ç¡®å®šç«¯å£
    port = args.port if args.port else int(os.environ.get('PORT', 8000))
    logger.info(f'å¯åŠ¨æœåŠ¡å™¨ï¼Œç›‘å¬ç«¯å£: {port}')

    app.run(host='0.0.0.0', port=port, debug=False)
